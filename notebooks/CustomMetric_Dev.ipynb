{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "Can we define the custom metric in Keras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_macro(y_true, y_pred):\n",
    "    \n",
    "    y_true = K.cast(y_true, tf.float64)\n",
    "    y_pred = K.cast(y_pred, tf.float64)\n",
    "    \n",
    "    TP = K.sum(y_true * K.round(y_pred), axis=0)\n",
    "    FN = K.sum(y_true * (1 - K.round(y_pred)), axis=0)\n",
    "    FP = K.sum((1 - y_true) * K.round(y_pred), axis=0)\n",
    "    \n",
    "    prec = TP / (TP + FP)\n",
    "    rec = TP / (TP + FN)\n",
    "    \n",
    "    # Convert NaNs to Zero\n",
    "    prec = tf.where(tf.is_nan(prec), tf.zeros_like(prec), prec)\n",
    "    rec = tf.where(tf.is_nan(rec), tf.zeros_like(rec), rec)\n",
    "    \n",
    "    f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    \n",
    "    # Convert NaN to Zero\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    f1 = K.mean(f1)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Conv2D, Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Builds the testModel1 Architecture.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(264, 264, 4))\n",
    "    x = Conv2D(64, (3, 3), activation=tf.nn.relu)(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    for i in range(2):\n",
    "        x = Conv2D(64, (3, 3), activation=tf.nn.relu)(x)\n",
    "        x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(264, activation=tf.nn.relu)(x)\n",
    "    outputs = Dense(28, activation=tf.nn.sigmoid)(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.001)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics={'f1_macro' : f1_macro})\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Data\n",
    "For now we're just using some random numbers rather than real data, since we just want to know if the custom metric is actually being calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "random_data_train = np.random.rand(100, 264, 264, 4)\n",
    "val_data_train = np.random.rand(100, 264, 264, 4)\n",
    "\n",
    "\n",
    "true_labels_train = np.round(np.random.rand(100, 28))\n",
    "val_label_train = np.round(np.random.rand(100, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 5s 48ms/step - loss: 1.2438 - val_loss: 0.7157\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(random_data_train, true_labels_train, validation_data=(val_data_train, val_label_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where the hell is the F1-Macro calculation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.7156828784942627], 'loss': [1.2438363027572632]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_macro': <function __main__.f1_macro(y_true, y_pred)>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... The metric is there... but where is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7052780342102051"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(random_data_train, true_labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the loss, but the metric isn't there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Custom Metric Calculates Stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(random_data_train)\n",
    "rounded_preds = np.round(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/.local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3206456974113276"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## From sklearn\n",
    "f1_score(true_labels_train, rounded_preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom\n",
    "f1 = f1_macro(true_labels_train, rounded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32064569741132765"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, identical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
